setwd("/Users/macbookair/Desktop/STQD6114 - Unstructured Data Analysis")

gc1 <- read.table("GC.txt", fill = T, header = F)
gc1
gc1[1,]

gc2 <- read.csv("GC.csv", header = F)
gc2
gc2[1,]
str(gc2)

## using tm package ##
# need to load NLP library before using tm package
library(NLP)
library(tm)

eg3 <- c("Hi!","Welcome to STQD6114","Tuesday, 11-1pm")
mytext <- VectorSource(eg3)
mycorpus <- VCorpus(mytext)
inspect(mycorpus)
as.character(mycorpus[[1]])
as.character(mytext[1])

# read txt file using tm package
# to convert a vector data into a corpus data (remeber! identify the source type): 
gc4 <- t(gc1)
a <- sapply(1:7, function(x) trimws(paste(gc4[, x], collapse = " "), "right"))
mytexta <- VectorSource(a)
mycorpusa <- VCorpus(mytexta)
inspect(mycorpusa)
as.character(mycorpusa[[1]])


# read csv file using tm package

gc4b <- t(gc2) #tranpose first so that each line is represented by columns and not rows
b <- sapply(1:6, function(x) trimws(paste(gc4b[, x], collapse = " "), "right"))
mytextb <- VectorSource(b)
mycorpusb <- VCorpus(mytextb)
inspect(mycorpusb)
as.character(mycorpusb[[1]])

# read csv file for Rise.csv
rise1 <- read.csv("Rise.csv")
riset <- t(rise1)
rise2 <- sapply(1:7, function(x) trimws(paste(riset[, x], collapse = " "), "right"))
mytextrise <- VectorSource(rise2)
mycorpusrise <- VCorpus(mytextrise)
inspect(mycorpusrise)
as.character(mycorpusrise[[1]])

##Using doc6.csv
eg5<-read.csv("Doc6.csv") 
docs<-data.frame(doc_id=c("doc_1","doc_2"),
                 text=c(as.character(eg5[1,]),as.character(eg5[2,])),
                 dmeta1=1:2,dmeta2=letters[1:2],stringsAsFactors=F)
mytext3 <- DataframeSource(docs)
mycorpus3 <- VCorpus(mytext3)
inspect(mycorpus3)
as.character(mycorpus3[[1]])


# use gc2 (CG.csv)
docs<-data.frame(doc_id=c("doc_1","doc_2", "doc_3", "doc_4", "doc_5", "doc_6"),
                 text=c(as.character(gc2[1,]),as.character(gc2[2,]),as.character(gc2[3,]),
                        as.character(gc2[4,]),as.character(gc2[5,]),as.character(gc2[6,])),
                 dmeta1=1:6,dmeta2=letters[1:6],stringsAsFactors=F)
mytext4 <- DataframeSource(docs)
mycorpus4 <- VCorpus(mytext4)
inspect(mycorpus4)
as.character(mycorpus4[[1]])

## example using DirSource (using file directly if got multiple files)
mytextmv<-DirSource("/Users/macbookair/Desktop/STQD6114 - Unstructured Data Analysis/movies")
mycorpusmv<-VCorpus(mytextmv)
inspect(mycorpusmv)
as.character(mycorpusmv[[1]]) #shows 3 lines because it belongs to the first document in the file




###### week 3 #######

## Web Scraping ##

# extract wikipedia content
eg6 <- readLines("https://en.wikipedia.org/wiki/Data_science")
eg6[grep("\\h2", eg6)]
eg6[grep("\\p", eg6)]

# using library XML
library(XML)
doc <- htmlParse(eg6) #reads entire document
doc.text<-unlist(xpathApply(doc,'//p',xmlValue)) #extracts certain parts: //p = paragraph
unlist(xpathApply(doc,'//h2',xmlValue)) # //h2 = 2nd level header

# Using library httr
library(httr)
eg7<-GET("https://www.edureka.co/blog/what-is-data-science/")
doc2<-htmlParse(eg7)
doc2.text<-unlist(xpathApply(doc2,'//p',xmlValue))
unlist(xpathApply(doc2,'//h2',xmlValue))


# practice using different website (non-wikipedia)
blog2 <- readLines("https://www.edureka.co/blog/what-is-data-science/")
blog2doc <- htmlParse(blog2) #reads entire document
blog2doc.text<-unlist(xpathApply(blog2doc,'//p',xmlValue)) #extracts certain parts: //p = paragraph
unlist(xpathApply(blog2doc,'//h2',xmlValue))

# practice using barbie wikipedia
barbie1 <- readLines("https://en.wikipedia.org/wiki/Barbie")
barbiedoc <- htmlParse(barbie1) #reads entire document
barbiedoc.text<-unlist(xpathApply(barbiedoc,'//p',xmlValue)) #extracts certain parts: //p = paragraph
unlist(xpathApply(barbiedoc,'//h2',xmlValue)) # //h2 = 2nd level header
#
barbie2 <- GET("https://en.wikipedia.org/wiki/Barbie")
barbiedoc2 <- htmlParse(barbie2)
barbiedoc2.text <- unlist(xpathApply(barbiedoc2,'//h2',xmlValue))


#Using library rvest
library(rvest)
eg8<-read_html("https://www.edureka.co/blog/what-is-data-science/")
nodes<-html_nodes(eg8,'ul+ p , p span') #use select tool extension, select a part to extract, then paste the html code into the "_____" after *eg8*
texts<-html_text(nodes)

#Selecting multiple pages
pages<-paste0('https://www.amazon.com/s?k=cosrx&crid=17R5NCU0TJ9LI&sprefix=cosrx%2Caps%2C337&ref=nb_sb_noss_1&page=',0:1) # 0:1 = page 1-2, if 0:9 = pg 1-10 (like python)
eg10<-read_html(pages[1]) #one page content only ******
nodes<-html_nodes(eg10,'.puis-medium-weight-text')
texts<-html_text(nodes)

nodes2 <- html_nodes(eg10, ".aok-align-bottom")
texts2 <- html_text(nodes2)
nodes3 <- html_nodes(eg10, ".a-price-fraction , .a-price-whole")
texts3 <- html_text(nodes3)


Price<-function(page) #many pages content ******
  {url <- read_html(page)
  nodes <- html_nodes(url ,'.a-price-whole ')
  html_text(nodes)}

sapply(pages,Price)
do.call("c",lapply(pages,Price))

# test on innisfree shopee #cannot do on shopee :(
pages2<-paste0('https://shopee.com.my/search?keyword=innisfree&trackingId=searchhint-1711523999-6ba2571f-ec0a-11ee-bca6-de94005d885d&page=',0:4) # 0:1 = page 1-2, if 0:9 = pg 1-10 (like python)
eg10_2<-read_html(pages[1]) #one page content only ******
nodes<-html_nodes(eg10_2,'.TnwXze , ._6HM3s0')
texts<-html_text(nodes)




